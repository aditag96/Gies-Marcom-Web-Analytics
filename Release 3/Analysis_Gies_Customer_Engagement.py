# -*- coding: utf-8 -*-
"""Program_Engagement.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h-sLSOFgWvQZUI8zo8wH3lWr1-hhXoy-
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re  # Import for regular expressions

df1 = pd.read_excel('Tech_Info_online.xlsx')
df2 = pd.read_excel('Tech_Info_business.xlsx')

# Function to detect garbled text (add this)
def contains_garbled_text(text):
    if pd.isnull(text):
        return False
    return not all(ord(c) < 128 for c in text)

# Calculate the number and percentage of garbled rows
garbled_rows_df1 = df1['Page title'].apply(contains_garbled_text).sum()
garbled_percentage_df1 = (garbled_rows_df1 / len(df1)) * 100

garbled_rows_df2 = df2['Page title'].apply(contains_garbled_text).sum()
garbled_percentage_df2 = (garbled_rows_df2 / len(df2)) * 100

# Print the results
print(f"Number of rows with garbled text in Tech_Info_online.xlsx: {garbled_rows_df1}")
print(f"Percentage of rows with garbled text in Tech_Info_online.xlsx: {garbled_percentage_df1:.2f}%")
print(f"Number of rows with garbled text in Tech_Info_business.xlsx: {garbled_rows_df2}")
print(f"Percentage of rows with garbled text in Tech_Info_business.xlsx: {garbled_percentage_df2:.2f}%")

# Drop rows with garbled page titles (add these lines)
df1 = df1[~df1['Page title'].apply(contains_garbled_text)]
df2 = df2[~df2['Page title'].apply(contains_garbled_text)]

# Remove rows with NaN in essential columns (like 'Date')
df1 = df1.dropna(subset=['Date'])
# Convert Date to datetime format
df1['Date'] = pd.to_datetime(df1['Date'].astype(int), format='%Y%m%d', errors='coerce')
df1 = df1.dropna(subset=['Date']) # Drop rows where date conversion failed

# Check for any other missing values
print(df1.isnull().sum())

# Remove rows with NaN in essential columns (like 'Date')
df2 = df2.dropna(subset=['Date'])
# Convert Date to datetime format
df2['Date'] = pd.to_datetime(df2['Date'].astype(int), format='%Y%m%d', errors='coerce')
df2 = df2.dropna(subset=['Date']) # Drop rows where date conversion failed

# Check for any other missing values
print(df2.isnull().sum())

# Define classification logic
def classify_program(title):
    if not isinstance(title, str):
        return 'Other'
    title = title.lower()
    if 'business analytics' in title or 'msba' in title:
        return 'MSBA'
    elif 'finance' in title or 'msf' in title:
        return 'MSF'
    elif 'accounting' in title or 'msa' in title:
        return 'MSA'
    elif 'management' in title and 'technology' not in title:
        return 'MSM'
    elif 'technology management' in title or 'mstm' in title:
        return 'MSTM'
    else:
        return 'Other'

# Apply classification
df2['Program'] = df2['Page title'].apply(classify_program)

# Group by Date and Program to count engagements
daily_engagement = df2.groupby(['Date', 'Program']).size().reset_index(name='Engagements')

df2

# Recalculate daily engagement using 'Views'
daily_engagement_views = df2.groupby(['Date', 'Program'])['Views'].sum().reset_index(name='Engagements')

# Compute total views per program
total_views = daily_engagement_views.groupby("Program")["Engagements"].sum().reset_index()
total_views = total_views.rename(columns={"Engagements": "Total Engagements"})

# Merge total into daily data
merged_views_df = pd.merge(daily_engagement_views, total_views, on="Program")

# Sort for clarity
merged_views_df = merged_views_df.sort_values(by=["Program", "Date"])

program_totals_sorted = merged_views_df[['Program', 'Total Engagements']].drop_duplicates().sort_values(by='Total Engagements', ascending=False)
program_totals_sorted

# Remove 'Other' category from the data
filtered_views_df = merged_views_df[merged_views_df['Program'] != 'Other']

# Plot daily engagement trends without 'Other'
plt.figure(figsize=(14, 8))
sns.lineplot(data=filtered_views_df, x="Date", y="Engagements", hue="Program", marker="o")

# Customize the plot
plt.title("Daily Engagement Trends by GIES Graduate Program (Excluding 'Other')", fontsize=16)
plt.xlabel("Date")
plt.ylabel("Engagements (Views)")
plt.legend(title="Program")
plt.xticks(rotation=45)
plt.tight_layout()

# Display the plot
plt.show()

merged_views_df

# For each program, calculate total views by page title and select top 5 and bottom 5
summary_by_program = []

for program in df2['Program'].unique():
    if program == 'Other':
        continue
    prog_df = df2[df2['Program'] == program]
    views_summary = prog_df.groupby('Page title')['Views'].sum().reset_index()
    views_summary = views_summary.sort_values(by='Views', ascending=False)

    top_5 = views_summary.head(5).assign(Rank='Top 5', Program=program)
    bottom_5 = views_summary.tail(5).assign(Rank='Bottom 5', Program=program)

    summary_by_program.append(pd.concat([top_5, bottom_5]))

page_view_extremes = pd.concat(summary_by_program)[['Program', 'Rank', 'Page title', 'Views']]
page_view_extremes = page_view_extremes.sort_values(by=['Program', 'Rank', 'Views'], ascending=[True, True, False])

# Arrange so that Top 5 rows appear before Bottom 5 for each program
page_view_extremes['Rank'] = pd.Categorical(page_view_extremes['Rank'], categories=['Top 5', 'Bottom 5'], ordered=True)
page_view_extremes_sorted = page_view_extremes.sort_values(by=['Program', 'Rank', 'Views'], ascending=[True, True, False])

# Print again with Top 5 appearing above Bottom 5 for each program
for program in page_view_extremes_sorted['Program'].unique():
    print(f"\n=== {program} ===")
    print(page_view_extremes_sorted[page_view_extremes_sorted['Program'] == program][['Rank', 'Page title', 'Views']].to_string(index=False))